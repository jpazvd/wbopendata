% =====================================================================
% Stata Journal-style LaTeX manuscript for wbopendata
% RESTRUCTURED VERSION following SJ conventions (fewer sections)
% =====================================================================

\documentclass[bib]{statapress}

\usepackage[crop,newcenter,frame]{pagedims}
\usepackage{sj_clean}
\usepackage{stata}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{graphicx}
\usepackage{float}
\usepackage{fancyvrb}
\usepackage[hidelinks]{hyperref}

\fvset{fontsize=\fontsize{8}{9}\selectfont, xleftmargin=12pt}

\newcommand{\floatnote}[2][\linewidth]{%
  \par\smallskip
  \begin{minipage}{#1}
    \small\raggedright #2
  \end{minipage}
}

\sjsetissue{$vv$}{$ii$}{$mm$}{$yyyy$}

\inserttype[st0001]{article}

\title[Data Provenance in the Age of Automation]{Data Provenance in the Age of Automation: Lessons from Fifteen Years of Programmatic Access to World Bank Open Data}

\author{Jo\~{a}o Pedro Azevedo}{%
Jo\~{a}o Pedro Azevedo\\
jpazvd.github.io%
}

\begin{document}
\maketitle

% =====================================================================
\begin{abstract}
This article reflects on fifteen years of \texttt{wbopendata}, the first Stata command to provide programmatic access to an international development data repository. Today, \texttt{wbopendata} provides access to over 29{,}000 indicators from 51 databases spanning 296 countries and aggregates, with features including five download modes, multilingual metadata, and publication-ready graph formatting. Its broader contribution lies in treating data acquisition as code: indicator selections, country filters, and time ranges become explicit parameters in analysis scripts rather than undocumented manual downloads---addressing the data provenance dimension of the reproducibility crisis that statistical reforms alone cannot fix. Yet the command has never been more relevant. As AI tools accelerate analytical workflows while enabling plausible fabrication of statistics and citations, anchoring research to authoritative, version-controlled data sources becomes essential infrastructure---not legacy convenience. I document the command's latest syntax and stored results, demonstrate workflows from basic queries to choropleth mapping, and present a 44-scenario test suite. I also discuss risks that frictionless data access can obscure---provenance opacity, coverage gaps masked by convenient defaults, and sustainability pressures---alongside mitigation strategies. I distill three design principles from fifteen years of sustained use: backward compatibility builds trust, domain-specific syntax lowers barriers, and scripted data access makes reproducibility the default rather than the exception.

\keywords{Stata, Open Data, World Bank, APIs, reproducible research, development indicators, metadata}
\end{abstract}


% =====================================================================
\section{Introduction}\label{sec:intro}
% =====================================================================

In April 2010, the World Bank launched its Open Data Initiative \citep{worldbank2010opendata},
transforming decades of development statistics into a freely accessible global public good.
By coupling an open data portal with a programmatic Application Programming Interface (API),
the initiative reframed official statistics from downloadable artifacts into queryable services.
This shift altered not only how data are disseminated, but how empirical research can be
conducted.

Within less than a year, \texttt{wbopendata} \citep{azevedo2011wbopendata} was released as a
Stata command translating this new API into a domain-specific interface for applied researchers.
Over the subsequent fifteen years, both the World Bank’s data infrastructure and
\texttt{wbopendata} evolved substantially: the number of databases expanded, legacy endpoints
were retired, metadata standards matured, and new requirements emerged around multilingual
documentation, versioning, and reproducibility. Throughout these changes, \texttt{wbopendata}
maintained backward compatibility while embedding increasingly sophisticated mechanisms for
scripted data access, metadata management, and automated documentation.

This article advances three claims. First, a substantial share of the reproducibility crisis in
the social sciences stems not from statistical methodology, but from opaque data acquisition
practices. Manual downloads, undocumented filters, and ad hoc preprocessing sever the link
between published results and their underlying data sources. Second, treating data acquisition
as code—where indicator selection, country coverage, and time ranges are explicit,
parameterized, and executable—offers a practical response to this problem. \texttt{wbopendata}
operationalizes this principle by making data provenance an integral component of the analytical
script rather than an external narrative. Third, in an era of rapidly expanding AI-assisted
research, such constraints are becoming more, not less, important. As generative tools lower the
cost of producing plausible analyses and narratives, anchoring empirical work to authoritative
and verifiable data sources becomes essential infrastructure for scientific credibility.

Against this backdrop, \texttt{wbopendata} is best understood not as a convenience tool, but as a
constraint mechanism. By binding analysis to a single authoritative API and exposing all data
selection decisions as executable code, it limits the space of admissible data inputs in ways
that complement—but also discipline—AI-assisted workflows. The command ensures that acceleration
downstream does not come at the cost of unverifiable or opaque data upstream.

The remainder of the article documents the current capabilities of \texttt{wbopendata},
demonstrates reproducible analytical workflows, and reflects on lessons learned from fifteen
years of sustained use. Section~\ref{sec:design} introduces the design principles and scope of the command.
Section~\ref{sec:command} documents its syntax and stored results.
Section~\ref{sec:workflows} presents canonical workflows for reproducible analysis and visualization.
Section~\ref{sec:reliability} describes the technical implementation and test suite.
Section~\ref{sec:discussion} discusses broader implications for reproducibility in the context of AI-assisted research,
and Section~\ref{sec:conclusion} concludes.


%===============================================================================
% Section: Design Principles and Scope  (SJ style, ready to paste)
% Recommended placement: after Introduction, before "The wbopendata command"
% If you insert as new Section 2, renumber subsequent sections accordingly.
%===============================================================================

\section{Design principles and scope}\label{sec:design}
The fifteen-year evolution of \texttt{wbopendata} reflects design choices shaped by
sustained use in applied research environments rather than abstract software
engineering goals. These choices respond to practical constraints common in work
with official statistics: heterogeneous upstream producers, revisions to published
series, institutional computing restrictions, and the need to document provenance
transparently. This section distills three design principles embedded in the command
and clarifies its intended scope.

\subsection{Data acquisition as code}
The foundational principle of \texttt{wbopendata} is that data acquisition should be
treated as part of the analytical codebase rather than as an external preparatory step.
Indicator selection, country coverage, time ranges, and filters are expressed as explicit
command parameters that can be executed, inspected, and version-controlled alongside
the analysis itself.

This approach contrasts with common workflows in which researchers manually download
spreadsheets from web portals, rename files, apply undocumented filters, and import the
results into statistical software. Such workflows often leave no complete record of how
the analytical dataset was constructed, making replication difficult even when code and
data files are shared. By encoding acquisition decisions directly in Stata syntax,
\texttt{wbopendata} produces an executable specification of provenance: a command line
documents what data were requested, from which source, and under what constraints.

This principle does not guarantee identical data across time. Instead, it guarantees
traceability. If upstream data are revised, the same command yields systematically
updated results rather than silently diverging datasets. Reproducibility, in this sense,
includes both exact replication (when sources are unchanged) and transparent updating
(when new observations or revisions are introduced).

\subsection{Backward compatibility as trust infrastructure}
A second guiding principle is the prioritization of backward compatibility. Over the
past decade, the World Bank API has undergone substantial changes, including endpoint
deprecations, catalog restructuring, and metadata revisions. Throughout these transitions,
\texttt{wbopendata} has aimed to preserve stable syntax and predictable behavior wherever
possible.

Backward compatibility serves a methodological function beyond user convenience.
Researchers build analytical pipelines that may be rerun years after initial publication,
often by different teams. When data access commands change semantics or fail without
warning, reproducibility is compromised even if the original code is preserved. By
absorbing API changes within the command's internal logic---rather than propagating them
to the user interface---\texttt{wbopendata} reduces the risk that historical analyses become
irreproducible due to infrastructure drift.

This commitment imposes constraints. New features are introduced cautiously, defaults are
chosen conservatively, and deprecated behavior is handled through informative diagnostics.
The result is a command that evolves incrementally while maintaining continuity, fostering
long-term trust among users who rely on it as part of production workflows.

\subsection{Domain-specific syntax as error prevention}
The third design principle is the use of domain-specific syntax tailored to applied
development research. Rather than exposing users directly to HTTP requests, JSON parsing,
pagination logic, or API schemas, \texttt{wbopendata} presents an interface organized around
concepts familiar to its audience: indicators, countries, years, topics, and metadata.

This choice is not merely ergonomic. By constraining user input to semantically meaningful
parameters, the command reduces opportunities for error and misinterpretation. Indicator
codes must be valid; country identifiers must conform to documented standards; time ranges
are explicit. These constraints limit the space of admissible queries and make incorrect
or ambiguous data requests easier to detect.

In this sense, domain-specific syntax functions as a guardrail. While generic API clients
offer maximal flexibility, they also place the burden of correctness entirely on the user.
\texttt{wbopendata} trades some generality for a higher likelihood that queries correspond
to interpretable, well-documented data products.

\subsection{Scope, boundaries, and diffusion}

\subsubsection*{Scope and non-goals}
Clarifying scope is essential to understanding the role of \texttt{wbopendata} in
reproducible research. The command is designed to provide programmatic access to
\emph{aggregated} development indicators disseminated through the World Bank API,
together with their associated metadata. It is not a tool for accessing confidential or
licensed microdata, nor does it perform statistical harmonization, imputation, or
methodological adjustments beyond those already embodied in the source databases.

Similarly, \texttt{wbopendata} does not aim to abstract away substantive judgment. Choices
about indicator suitability, methodological breaks, coverage gaps, and interpretation
remain the responsibility of the researcher. The command supports these judgments by
surfacing metadata and coverage diagnostics, but it does not substitute for domain
expertise.

Within these boundaries, \texttt{wbopendata} is best understood as infrastructure rather
than analysis software. Its contribution lies in constraining and documenting the earliest
stage of the empirical workflow---data acquisition---so that subsequent analysis, whether
conducted manually or assisted by automated tools, rests on verifiable and reproducible
foundations.

\subsubsection*{Diffusion beyond \texttt{wbopendata}}
The design principles outlined above are not unique to \texttt{wbopendata}. They
reflect a broader pattern of tooling that has emerged within applied development
research to address reproducibility, provenance, and scale. In particular, similar
principles have guided the development of other Stata-based data access tools,
both within and beyond the World Bank.

Within the World Bank, these ideas informed the evolution of internal data
infrastructure such as \texttt{datalib}, \texttt{datalib2}, and \texttt{datalibweb} \citep{datalibweb2018},
which provide structured, version-controlled access to raw and harmonized household survey
microdata. While differing substantially in scope and access restrictions from
\texttt{wbopendata}, these tools apply the same core logic: data retrieval is scripted,
parameterized, and embedded directly in analytical workflows. This approach has enabled large-scale, reproducible analytical systems, most notably
the Poverty and Inequality Platform (PIP) \citep{WorldBankPIP}, a publicly accessible
global platform that disseminates official poverty and inequality estimates produced
through transparent and replicable pipelines.

Outside the World Bank, related principles can be observed in user-written Stata
tools such as \texttt{datazoom} \citep{datazoom2020}, which systematizes the preparation of Brazilian
household survey microdata produced by the national statistical agency. Although
\texttt{datazoom} operates on locally obtained files rather than remote APIs, it
similarly emphasizes standardized structure, transparent preprocessing, and
reusable code as prerequisites for credible empirical analysis.

More recently, the same design logic has been extended to multi-language data
access through \texttt{unicefData} \citep{unicefdata2024}, which provides a triangulated suite of R, Python,
and Stata interfaces to UNICEF’s SDMX-based data warehouse. While implemented in
different programming environments, these tools share a common emphasis on
explicit data acquisition, metadata exposure, and reproducible defaults.

These examples are not intended as an exhaustive survey of data infrastructure
tools, nor do they imply architectural equivalence. Rather, they illustrate that the
principles embedded in \texttt{wbopendata} have proven portable across institutions,
data modalities, and governance contexts. Their recurrence suggests that treating
data access as constrained, executable infrastructure—rather than an informal
preprocessing step—is a generalizable response to the reproducibility challenges
facing empirical research with official statistics.

% =====================================================================
\section{The wbopendata command}\label{sec:command}
% =====================================================================

The databases accessible through \texttt{wbopendata} include World Development Indicators (WDI), Doing Business, Worldwide Governance Indicators, International Debt Statistics, Africa Development Indicators, Education Statistics, Enterprise Surveys, Gender Statistics, Health Nutrition and Population Statistics, Global Financial Inclusion (Findex), Poverty and Equity, Human Capital Index, Sustainable Development Goals, and many more. Table~\ref{tab:coverage} summarizes the current scope.

\begin{table}[H]
\centering
\caption{World Bank Open Data coverage}
\label{tab:coverage}
\begin{tabular}{ll}
\hline
Dimension & Coverage \\
\hline
Indicators & 29{,}000+ \\
Data sources & 51 databases \\
Topic categories & 21 \\
Countries \& regions & 296 \\
Country attributes & 17 \\
Time coverage & 1960--present \\
Languages & 3 (English, Spanish, French) \\
\hline
\end{tabular}
\end{table}

\texttt{wbopendata} talks directly to the World Bank API (JSON over HTTP), returns tidy Stata datasets, and caches results to minimize repeat downloads. Five pull modes cover country, topic, single-indicator (all countries), single-indicator (selected countries), and multi-indicator requests. Output may be wide or long; \texttt{latest} works in long mode and returns ready-to-use scalars for titles and subtitles. Metadata is always fetched; v17.7.1 adds basic country context (region, admin region, income level, lending type) by default.

\subsection{Syntax}

\begin{stsyntax}
\dunderbar{wb}opendata,\
\{\
\underbar{ind}icator(\ststring) $\mid$\
\underbar{c}ountry(\ststring) $\mid$\
\underbar{top}ics(\ststring)\/\
\}\
\optional{options}
\end{stsyntax}

\subsubsection*{Data selection}

Exactly one of the following is required:

\hangpara
\texttt{\underbar{ind}icator(\ststring)} specifies one or more World Bank indicator codes. Multiple indicators can be requested by separating codes with semicolons, for example, \texttt{indicator(SP.POP.TOTL;NY.GDP.PCAP.CD)}.

\hangpara
\texttt{\underbar{c}ountry(\ststring)} specifies ISO3 country codes or World Bank region codes to retrieve all available indicators for selected countries. Multiple codes can be separated by semicolons.

\hangpara
\texttt{\underbar{top}ics(\num)} specifies a topic ID (1--21) to retrieve all indicators within a thematic category such as education, health, or environment.

\subsubsection*{Time and language}

\hangpara
\texttt{\underbar{y}ear(\ststring)} restricts the time interval. For example, \texttt{year(2000:2020)} returns data only for years 2000 through 2020.

\hangpara
\texttt{\underbar{l}anguage(\ststring)} sets the language for metadata display. Valid codes are \texttt{en} (English, default), \texttt{es} (Spanish), and \texttt{fr} (French).

\hangpara
\texttt{\underbar{proj}ection} accesses population estimates and projections from the Health Nutrition and Population Statistics database rather than actual census data.

\subsubsection*{Output format}

\hangpara
\texttt{long} returns data in long format with one row per country-year. The default is wide format with year-specific columns (\texttt{yr1960}, \texttt{yr1961}, \ldots).

\hangpara
\texttt{clear} replaces any data currently in memory. Required if data are already loaded.

\hangpara
\texttt{\underbar{lat}est} keeps only the most recent non-missing observation per country. Requires the \texttt{long} option. When multiple indicators are requested, retains only observations where \textit{all} indicators have non-missing values in the same year.

\hangpara
\texttt{\underbar{desc}ribe} displays indicator metadata without downloading data. Useful for exploring indicator definitions and sources before committing to a full download.

\hangpara
\texttt{\underbar{nomet}adata} suppresses the metadata display that normally appears after data retrieval.

\subsubsection*{Country attributes}

\hangpara
\texttt{\underbar{bas}ic} adds region, administrative region, income level, and lending type variables to the downloaded data. This is the default behavior in v17.7.1+.

\hangpara
\texttt{\underbar{nobas}ic} suppresses the default country attribute variables.

\hangpara
\texttt{\underbar{f}ull} adds all 17 country attributes including geographic coordinates and capital city. See Table~\ref{tab:attributes} for the complete list.

\hangpara
\texttt{\underbar{ge}o}, \texttt{capital}, \texttt{latitude}, \texttt{longitude} add specific geographic fields without the full set of attributes.

\hangpara
\texttt{\underbar{m}atch(\varname)} merges country attributes into an existing dataset. The variable \varname\ must contain World Bank country codes (ISO3 format).

\subsubsection*{Metadata management}

\hangpara
\texttt{\underbar{upd}ate query} displays the vintage dates of locally cached indicator and country metadata.

\hangpara
\texttt{\underbar{upd}ate check} compares local metadata against the remote repository and reports whether updates are available.

\hangpara
\texttt{\underbar{upd}ate all} downloads fresh metadata from the repository, replacing the local cache.

\subsubsection*{Graph metadata (v17.7.1+)}

\hangpara
\texttt{\underbar{linew}rap(\ststring)} wraps metadata text for use in graph titles. The argument specifies which metadata to wrap: \texttt{name}, \texttt{description}, \texttt{note}, \texttt{source}, \texttt{topic}, or \texttt{all}.

\hangpara
\texttt{\underbar{maxl}ength(\num)} sets the maximum characters per line for wrapped text. The default is 50.

\hangpara
\texttt{\underbar{linewrapf}ormat(\ststring)} controls the output format: \texttt{stack} (stacked lines), \texttt{newline} (newline-separated), \texttt{nlines} (returns line count), \texttt{lines} (returns individual lines), or \texttt{all} (returns all formats).

\subsection{Stored results}

\texttt{wbopendata} is an r-class command that stores results in \texttt{r()}. These stored results are critical for automation: they allow downstream code to programmatically access indicator metadata, construct dynamic graph titles, and build reproducible pipelines without manual intervention.

\textbf{Indicator codes and variable names.} World Bank indicator codes like \texttt{SI.POV.DDAY} contain periods, which Stata does not allow in variable names. The command automatically converts indicator codes to Stata-safe variable names by replacing periods with underscores and converting to lowercase: \texttt{SI.POV.DDAY} becomes \texttt{si\_pov\_dday}. Both forms are stored: \texttt{r(indicator\#)} preserves the original API code for documentation and re-querying, while \texttt{r(varname\#)} provides the Stata variable name for use in analysis commands.

\textbf{Indexed versus aggregate returns.} Results come in two forms. Indexed returns (\texttt{r(varname1)}, \texttt{r(varname2)}, \ldots) store metadata for each indicator separately, enabling indicator-specific labeling and citation. Aggregate returns store combined information: \texttt{r(indicator)} contains the full semicolon-separated query string as entered, while \texttt{r(name)} contains all variable names as a space-separated list suitable for \texttt{foreach} loops or variable lists.

For each requested indicator (indexed by \# = 1, 2, \ldots), the command returns:

\begin{table}[H]
\centering
\caption{Stored results}
\label{tab:stored}
\begin{tabular}{llp{6.5cm}}
\hline
Result & Type & Description \\
\hline
\multicolumn{3}{l}{\textit{Aggregate returns (always)}} \\
\texttt{r(indicator)} & local & Full query string (semicolon-separated) \\
\texttt{r(name)} & local & All Stata variable names (space-separated) \\
\hline
\multicolumn{3}{l}{\textit{Indexed returns (per indicator, always)}} \\
\texttt{r(indicator\#)} & local & Original API indicator code (e.g., SI.POV.DDAY) \\
\texttt{r(varname\#)} & local & Stata-safe variable name (e.g., si\_pov\_dday) \\
\texttt{r(varlabel\#)} & local & Indicator label from API \\
\texttt{r(source\#)} & local & Source database identifier \\
\texttt{r(time\#)} & local & Time dimension name \\
\texttt{r(sourcecite\#)} & local & Clean organization name (when Note is non-empty) \\
\hline
\multicolumn{3}{l}{\textit{With \texttt{year()} option}} \\
\texttt{r(year\#)} & local & Year or year range requested \\
\hline
\multicolumn{3}{l}{\textit{With \texttt{latest} option}} \\
\texttt{r(latest)} & local & Formatted subtitle string for graphs \\
\texttt{r(latest\_ncountries)} & local & Number of countries with data \\
\texttt{r(latest\_avgyear)} & local & Average year of observations \\
\texttt{r(latest\_year)} & local & Maximum year retained \\
\hline
\multicolumn{3}{l}{\textit{With \texttt{linewrap()} option}} \\
\texttt{r(name\#\_stack)} & local & Wrapped name for \texttt{title()} \\
\texttt{r(description\#\_stack)} & local & Wrapped description for captions \\
\texttt{r(note\#\_stack)} & local & Wrapped methodological notes \\
\texttt{r(source\#\_stack)} & local & Wrapped source text \\
\texttt{r(topic\#\_stack)} & local & Wrapped topic name \\
\texttt{r(*\#\_nlines)} & scalar & Line count for each field \\
\texttt{r(*\#\_line1)}, \ldots & local & Individual wrapped lines \\
\hline
\end{tabular}
\end{table}

These returns enable fully automated workflows: a script can download data, extract \texttt{r(name1\_stack)} for the graph title, \texttt{r(sourcecite1)} for the source note, and \texttt{r(latest)} for a coverage subtitle---all without hardcoding any metadata.

% =====================================================================
\section{Reproducible analytical workflows}\label{sec:workflows}
% =====================================================================

This section presents three canonical workflows that illustrate how
\texttt{wbopendata} supports reproducible empirical research. Rather than
exhaustively documenting every option, the workflows are selected to
demonstrate the command’s core design principles: scripted data acquisition,
metadata-driven annotation, and integration into larger analytical pipelines.
Extended examples and visualizations appear in Appendix~\ref{app:examples},
with diagnostics and QA artifacts in Appendices~\ref{app:diagnostics}
and~\ref{app:qa}.

\subsection{Scripted data acquisition and data shape}\label{sec:workflow-data}

The most fundamental workflow enabled by \texttt{wbopendata} is the scripted
retrieval of development indicators with explicit control over data shape.
The following command retrieves a single indicator for all countries and
returns the data in long format:

\begin{stlog}
. wbopendata, indicator(NY.GDP.MKTP.CD) long clear
\nullskip
\end{stlog}

This single line constitutes a complete, executable specification of data
provenance. The indicator definition, source database, country coverage, and
time dimension are all determined by the API query and documented through the
command’s stored results. Unlike manual downloads, no implicit filtering or
preprocessing occurs outside the script.

The choice between wide and long formats is not cosmetic. Long format aligns
naturally with Stata’s panel data commands, regression routines, and
\texttt{bysort} operations, while wide format may be preferable for descriptive
or tabular summaries. By making this choice explicit at the point of data
acquisition, \texttt{wbopendata} ensures that downstream analysis remains
transparent and reproducible. Full metadata output associated with this query
is reported in Appendix~\ref{app:examples}.

\subsection{Metadata-driven visualization}\label{sec:workflow-meta}

A second canonical workflow demonstrates how metadata retrieved programmatically
can be used directly to produce publication-ready visualizations. The
\texttt{latest} and \texttt{linewrap()} options enable the command to return
machine-readable metadata suitable for graph titles, captions, and source
annotations.

The example below retrieves two indicators, retains the most recent comparable
observation for each country, and prepares wrapped metadata for use in figures:

\begin{stlog}
. wbopendata, indicator(SI.POV.DDAY; SH.DYN.MORT) ///
    long latest linewrap(name description note)
\nullskip
\end{stlog}

The command stores formatted strings in \texttt{r()} that can be passed directly
to Stata’s graphing commands. Figure~\ref{fig:linewrap} illustrates this workflow by constructing
a scatter plot of poverty headcount ratios against under-five mortality rates,
with axis titles, definitions, and source notes populated automatically from
the returned metadata.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{figs/wbopendata_linewrap_example.pdf}
\caption{Poverty and child mortality scatter plot with automatic metadata annotation. Axis titles, definitions, and source citations are populated directly from \texttt{wbopendata}'s stored results using the \texttt{linewrap()} option.}
\label{fig:linewrap}

\floatnote[0.85\linewidth]{%
\emph{Note:} See Appendix~\ref{app:examples:viz:linewrap} for the script used to generate this figure.
}
\end{figure}


By treating metadata as structured output rather than narrative text,
\texttt{wbopendata} reduces transcription errors and ensures consistency between
data and documentation. In particular, the \texttt{latest} option records
coverage diagnostics---including the number of countries and the average
observation year---that can be displayed transparently in figure subtitles.
Alternative formatting options and full metadata returns are documented in
Appendix~\ref{app:examples}.

\subsection{Integration into analytical pipelines}\label{sec:workflow-pipeline}

Beyond single-command use, \texttt{wbopendata} is designed to function as
infrastructure within larger analytical pipelines. A prominent example is the
World Bank’s Learning Poverty project \citep{worldbank2019learningpoverty}, which computes global and regional
estimates of the share of children unable to read and understand a short text by
age ten.

In this pipeline, \texttt{wbopendata} is used to retrieve population weights,
enrollment rates, and auxiliary indicators from the World Development Indicators
database. These inputs are combined with harmonized learning assessment data in
a fully scripted workflow released under an open-source license with multiple
versioned releases. Because all external data dependencies are accessed
programmatically, the resulting estimates can be independently replicated and
systematically updated as new data become available.

This use case illustrates how the design principles described in Section~\ref{sec:design} scale
beyond exploratory analysis. By constraining data acquisition to explicit,
auditable commands, \texttt{wbopendata} enables complex, multi-step analytical
systems to remain reproducible over time, even as underlying data sources are
revised. Additional pipeline examples and related visualizations are provided in
Appendix~\ref{app:examples}.

% =====================================================================
\section{Technical implementation and reliability}\label{sec:reliability}
% =====================================================================

This section documents the technical implementation of \texttt{wbopendata} and
the mechanisms used to ensure reliability over time. The focus is deliberately
operational: how the command is installed, how it interacts with upstream data
infrastructure, and how stability is maintained as external APIs evolve.

\subsection{Installation}

The recommended installation method is via the Statistical Software Components
(SSC) archive:

\begin{stlog}
. ssc install wbopendata, replace
\nullskip
\end{stlog}

For users who require the latest development version, including recently added
metadata-handling features, the package can be installed directly from the
public repository:

\begin{stlog}
. net install wbopendata, ///
  from("https://raw.githubusercontent.com/jpazvd/wbopendata/main/src") replace
\nullskip
\end{stlog}

Both methods rely exclusively on Stata’s native package management system and
require no external dependencies.

\subsection{Architecture}

\texttt{wbopendata} is implemented entirely in Stata’s ado-file language and
communicates with the World Bank’s REST API endpoints \citep{worldbank2024api}.
Key architectural features include automatic pagination (retrieval and assembly
of multi-page API responses), deterministic caching using hashed request
parameters, and systematic normalization of indicator codes into legal Stata
variable names.

Metadata parsing and formatting build on established community utilities,
including \texttt{tknz} \citep{elliott2002tknz}, \texttt{linewrap}
\citep{overazevedo2000linewrap}, and \texttt{\_pecats}
\citep{longfreese2001pecats}. These components support string tokenization,
metadata wrapping, and category handling without introducing external binaries.

The implementation avoids third-party executables, ensuring compatibility with
institutional computing environments that restrict software installation.
Network requests respect Stata’s proxy configuration options (e.g.,
\texttt{set httpproxy on}), allowing the command to operate behind corporate
firewalls.

\subsection{Country attributes and classification}

When invoked with the \texttt{full} option, \texttt{wbopendata} appends a set of
17 country attributes that support classification, merging, and stratified
analysis. Table~\ref{tab:attributes} summarizes these variables.

\begin{table}[H]
\centering
\caption{Country attributes returned with \texttt{full} option}
\label{tab:attributes}
\begin{tabular}{ll}
\hline
Variable & Description \\
\hline
\texttt{countrycode} / \texttt{countryname} & ISO3 code and name \\
\texttt{region} / \texttt{regionname} & Region code and name \\
\texttt{adminregion} / \texttt{adminregionname} & Administrative region \\
\texttt{incomelevel} / \texttt{incomelevelname} & Income classification \\
\texttt{lendingtype} / \texttt{lendingtypename} & Lending type (IBRD, IDA, Blend) \\
\texttt{capital} & Capital city name \\
\texttt{latitude} / \texttt{longitude} & Capital coordinates \\
\hline
\end{tabular}
\end{table}

These attributes can also be merged into existing datasets using the
\texttt{match(varname)} option, where \texttt{varname} contains World Bank
country codes. This enables enrichment without repeated data downloads.

Regional aggregates are identified explicitly via the \texttt{region} variable,
allowing users to include or exclude aggregate observations using transparent
filters (e.g., \texttt{keep if region != "NA"} for individual countries).

\subsection{Testing and error handling}

To ensure reliability, \texttt{wbopendata} ships with a live integration test
harness that exercises the same workflows encountered by users in practice.
Tests are executed end-to-end against the live World Bank API, covering
indicator retrieval, pagination, caching behavior, option handling, and fixes
for historical issues.

Because Stata lacks the mature continuous integration and mocking frameworks
available in other statistical environments, the test design prioritizes
detection of upstream regressions such as schema changes, pagination failures,
or API endpoint deprecations. The test suite comprises 44 integrated tests
distributed across 13 categories, summarized in Table~\ref{tab:test-coverage}.

\begin{table}[H]
\centering
\caption{Test suite composition (44 tests across 7 categories)}
\label{tab:test-coverage}

\begin{tabular}{llrl}
\hline
Abbr. & Category & Tests & Focus \\
\hline
ENV  & Environment     & 2  & Network connectivity, API availability \\
DL   & Download modes  & 8  & Indicator, country, topic, multi-indicator \\
FMT  & Output format   & 5  & Wide, long, reshape, latest \\
CTRY & Country options & 4  & Basic, full, geo, ISO \\
LW   & Linewrap        & 6  & Metadata wrapping, formats \\
REG  & Regression      & 7  & Historical bug fixes \\
ADV  & Advanced        & 12 & Edge cases, error handling \\
\hline
\end{tabular}

\floatnote{\emph{Note:} See Appendix~\ref{app:qa} for full test definitions and execution logic.}

\end{table}


When an indicator code is invalid or deprecated, \texttt{wbopendata} returns
informative diagnostics that guide users toward corrective action. Verbatim
examples of missing indicators, archived series, and update operations are
reported in Appendix~\ref{app:diagnostics}.

\subsection{Metadata management}

Local metadata caches are managed through the \texttt{update} options.
\texttt{update query} reports the current vintage, \texttt{update check}
compares local metadata against the remote repository, and \texttt{update all}
refreshes cached definitions and country classifications.

Regular metadata updates are particularly important when using the
\texttt{match()} option, as income groups, regional classifications, and even
country names may be revised over time. By making metadata management explicit,
\texttt{wbopendata} reduces the risk of silently propagating outdated
classifications into downstream analysis.

In addition to country classifications and descriptive fields, the set of
available indicator codes itself evolves over time. Indicators may be
deprecated, archived, or reclassified as source databases are revised, merged,
or retired, and metadata associated with existing series may be corrected or
updated. \texttt{wbopendata} treats these changes as first-class events rather
than silent failures. Deprecated indicators trigger explicit diagnostic
messages that identify the archival location, while revised metadata is
propagated through the local cache when updates are applied.

By making indicator availability and metadata revisions explicit, the command
reduces the risk that analytical pipelines continue to rely unknowingly on
obsolete series or outdated definitions. Verbatim examples illustrating
deprecated indicators and metadata updates are reported in Appendix~\ref{app:diagnostics}.

% =====================================================================
\section{Discussion: reproducibility and constraints in the age of AI}\label{sec:discussion}
% =====================================================================

Fifteen years after its initial release, \texttt{wbopendata} remains relevant not
because of any single technical feature, but because of the methodological
principles embedded in its design. As discussed in Section~\ref{sec:design}, these principles
treat data acquisition as executable infrastructure rather than an informal
preprocessing step. This section reflects on the implications of that design in
the context of contemporary empirical research, particularly as analytical
workflows are increasingly mediated by AI-assisted tools.

\subsection{Reproducibility beyond statistical methodology}

The reproducibility crisis in the social sciences has been widely documented
\citep{baker2016reproducibility,opensc2015estimating}. Much of the resulting reform
agenda has focused on statistical practice: pre-registration, replication
studies, robustness checks, and disclosure of code. While these efforts are
essential, they address only part of the problem. In many empirical workflows,
the construction of the analytical dataset itself remains opaque, relying on
manual downloads, undocumented filters, and ad hoc preprocessing that cannot be
fully reconstructed from code alone.

By treating data acquisition as code, \texttt{wbopendata} addresses this
dimension directly. A command specifying indicators, countries, and time ranges
constitutes a complete and executable description of how the analytical dataset
was assembled. When rerun, the same command either reproduces the original data
exactly or yields systematically updated results if upstream sources have been
revised. In both cases, the provenance of the data is explicit and auditable.
Reproducibility is therefore strengthened not by constraining statistical
analysis, but by constraining how data enter the analytical pipeline.

\subsection{Constraints as analytical infrastructure}

The increasing adoption of AI-assisted tools has altered the balance of effort
in empirical research. Tasks that were once time-consuming—coding, visualization,
and narrative drafting—can now be performed rapidly. This acceleration,
however, amplifies the importance of upstream constraints. When tools can
generate plausible-looking statistics, code, and citations, the credibility of
empirical work depends increasingly on the verifiability of its inputs.

In this environment, \texttt{wbopendata} functions less as a productivity
complement and more as a constraint mechanism. By binding analysis to a single,
authoritative API and requiring explicit specification of indicators, coverage,
and time periods, it limits the space of admissible data inputs. These constraints
do not inhibit analysis; rather, they discipline it by ensuring that downstream
automation operates on verifiable and well-documented sources. Acceleration is
thereby decoupled from fabrication: analytical workflows can scale without
relaxing standards of provenance.

This role is consistent with the design principles outlined in Section~\ref{sec:design}.
Backward compatibility preserves trust across time, domain-specific syntax
reduces opportunities for error, and scripted data access makes provenance the
default rather than an afterthought. Together, these features position
\texttt{wbopendata} as infrastructure that shapes how analysis is conducted,
rather than merely facilitating it.

\subsection{Generalizability and limits}

The principles embodied in \texttt{wbopendata} are not unique to development
indicators or to Stata. As discussed earlier, similar design logic underpins
other data-access tools operating on aggregated indicators and microdata, both
within and outside the World Bank. Their recurrence suggests that constraining
data access through executable specifications is a generalizable response to
reproducibility challenges in applied research.

At the same time, important limits remain. Programmatic access does not resolve
substantive issues of data quality, coverage gaps, or methodological breaks.
Indicators may be outdated, revised, or unavailable for political or technical
reasons. Moreover, secure access to confidential microdata continues to require
institution-specific governance arrangements that cannot be fully standardized.
\texttt{wbopendata} does not eliminate these challenges; it makes them visible by
exposing metadata, coverage diagnostics, and revision histories directly to the
user.

\subsection{Implications for open science}

The experience of \texttt{wbopendata} over fifteen years underscores a broader
lesson for open science: principles alone are insufficient without
infrastructure that embeds them into routine practice. Transparency and
reproducibility are widely endorsed norms, yet they are often undermined by
tooling that leaves critical steps implicit. By constraining data acquisition
through executable commands, tools like \texttt{wbopendata} shift reproducibility
from aspiration to default behavior.

In this sense, the contribution of \texttt{wbopendata} is methodological rather
than technological. It demonstrates that disciplined data access can coexist
with flexible analysis, and that constraining the earliest stages of the
workflow can enhance, rather than limit, analytical innovation. As empirical
research continues to evolve in the presence of increasingly powerful
automation, such constraint-based infrastructure will remain essential to
maintaining credibility and trust.

% =====================================================================
\section{Conclusion}\label{sec:conclusion}
% =====================================================================

Fifteen years after the World Bank Open Data Initiative transformed development
statistics into a global public good, \texttt{wbopendata} continues to function
as core infrastructure for reproducible empirical research in Stata. The command
provides stable, programmatic access to over 29{,}000 development indicators from
51 databases, encapsulating API communication, pagination, caching, and metadata
management within a single, backward-compatible interface.

This longevity reflects deliberate design choices rather than technical inertia.
By maintaining backward compatibility across upstream changes, encapsulating
complexity behind domain-specific syntax, and treating data acquisition as code
rather than manual preprocessing, \texttt{wbopendata} shifts reproducibility from
an aspirational norm to a default behavior. Version~17.7.1 extends this approach
with publication-ready metadata handling, default country-context attributes,
and strengthened support for multi-indicator workflows.

The command’s pure-Stata implementation ensures compatibility with institutional
computing environments, while the bundled live test suite and QA framework
demonstrate how reliability can be sustained even when upstream data systems
evolve. More broadly, the experience documented here illustrates that reproducible
analytics depends not only on statistical methods, but on disciplined data-access
infrastructure that makes provenance explicit, testable, and auditable.

In the age of AI-assisted research, this constraint-based approach is increasingly
important. As analytical workflows accelerate and the cost of producing plausible
statistics and narratives falls, credibility hinges on limiting the space of
admissible inputs to authoritative, verifiable sources. \texttt{wbopendata}
illustrates how such constraints can be embedded directly into routine research
practice—supporting scale and automation without relaxing standards of trust.

While developed in Stata and applied to World Bank data, the principles distilled
from fifteen years of use—scripted data acquisition, conservative evolution, and
operational validation—are not tool-specific. They point toward a broader lesson
for empirical research and open science: reproducibility is sustained not by
documentation alone, but by infrastructure that enforces it upstream.


% =====================================================================
\section*{Acknowledgments}
% =====================================================================

The author thanks the World Bank Open Data Initiative for making development data freely accessible, and the many users who have contributed bug reports and feature suggestions through GitHub.

\bibliographystyle{sj}
\nocite{pisati2007spmap,clarke2012worldstat}
\bibliography{wbopendata}

\begin{aboutauthors}
Jo\~{a}o Pedro Azevedo is Deputy Director and Chief Statistician in UNICEF's Division of Data, Analytics, Planning and Monitoring. Previously, he worked for 16 years at the World Bank as Lead Economist. His research focuses on poverty measurement, education statistics, and reproducible and scalable analytical pipelines for global, regional, and national monitoring systems to inform policy.

The software repository (\url{https://github.com/jpazvd/wbopendata}) includes example do-files, test scripts, and complete documentation.
\end{aboutauthors}


% =====================================================================
% Appendices: ALL sjlogs \input{} wrapped in stlog blocks
% =====================================================================

\appendix
% Ensure appendix sections are lettered (A, B, C)
\renewcommand{\thesection}{\Alph{section}}

% Appendix figure/table numbering: A1, A2, ... / Table A1, A2, ...
\setcounter{figure}{0}
\setcounter{table}{0}
\renewcommand{\thefigure}{\thesection\arabic{figure}}
\renewcommand{\thetable}{\thesection\arabic{table}}

% =====================================================================
% Appendix: Extended examples and visualizations
% =====================================================================
\section{Extended examples and visualizations}\label{app:examples}

This appendix provides extended examples, alternative visualizations, and
verbatim output referenced in the main text. These materials are included to
support full transparency and reproducibility while keeping the exposition in
Section~\ref{sec:workflows} focused on canonical analytical workflows.

% ---------------------------------------------------------------------
% A.1 Extended data retrieval examples
% ---------------------------------------------------------------------
\subsection{Extended data retrieval examples}\label{app:examples:data}

This subsection reports additional data retrieval examples illustrating
variations in indicator selection, output shape, and filtering options. These
examples expand on the scripted data acquisition workflow described in
Section~\ref{sec:workflow-data}.

\subsubsection{Single-indicator retrieval (verbatim output)}
\begin{stlog}
\input{sjlogs/ex_single_indicator.log.tex}\nullskip
\end{stlog}

\subsubsection{Multiple-indicator queries (verbatim output)}
The following example retrieves multiple indicators simultaneously and returns
the data in long format:
\begin{stlog}
\input{sjlogs/ex_multiple_indicators.log.tex}\nullskip
\end{stlog}

This pattern is commonly used in multivariate analysis and panel regressions,
where alignment across indicators and years is required.

\subsubsection{Country- and topic-based queries}
Additional examples demonstrate retrieval by country codes and topic identifiers,
illustrating how \texttt{wbopendata} supports exploratory analysis and bulk downloads
without manual file handling.

\begin{verbatim}
wbopendata, country(BRA; ARG; CHL) long clear
wbopendata, topics(11) clear
\end{verbatim}

Full metadata output associated with these queries is reported in
Appendix~\ref{app:examples:metadata} and Appendix~\ref{app:examples:viz}.

% ---------------------------------------------------------------------
% A.2 Extended metadata output
% ---------------------------------------------------------------------
\subsection{Extended metadata output}\label{app:examples:metadata}

This subsection reports verbatim metadata returned by \texttt{wbopendata},
including indicator names, definitions, sources, and topic classifications.
These outputs correspond to examples discussed in Sections~\ref{sec:workflow-data} and~\ref{sec:workflow-meta}.

\subsubsection{Indicator metadata (illustrative excerpt)}
\begin{verbatim}
Metadata for indicator SI.POV.DDAY
------------------------------------------------------------
Name: Poverty headcount ratio at $3.00 a day (2021 PPP)
Description: ...
Source: World Bank, Poverty and Inequality Platform
Topic(s): Poverty
------------------------------------------------------------
\end{verbatim}

Metadata is retrieved directly from the World Bank API and cached locally to
ensure consistency across repeated queries.

\subsubsection{Coverage diagnostics with \texttt{latest} (verbatim output)}
The following output illustrates coverage diagnostics returned by the
\texttt{latest} option, including the number of countries and the average
observation year:
\begin{stlog}
\input{sjlogs/ex_latest_option.log.tex}\nullskip
\end{stlog}

\subsubsection{\texttt{linewrap()} stored results (verbatim output)}
This output shows the wrapped metadata and stored results used for
metadata-driven graph annotation:
\begin{stlog}
\input{sjlogs/ex_linewrap_returns.log.tex}\nullskip
\end{stlog}

% ---------------------------------------------------------------------
% A.3 Extended visualizations (figures + supporting logs)
% ---------------------------------------------------------------------
\subsection{Extended visualizations}\label{app:examples:viz}

This subsection presents additional figures referenced but not reproduced in
the main text. These include alternative mappings, color schemes, and regional
breakdowns. Where relevant, the corresponding console output and commands are
provided verbatim.

\subsubsection{Metadata-driven graph annotation (supporting Figure~\ref{fig:linewrap})}\label{app:examples:viz:linewrap} 
\begin{stlog}
\input{sjlogs/ex_linewrap_graph.log.tex}\nullskip
\end{stlog}

\subsubsection{Choropleth mapping example (verbatim output)}
\begin{stlog}
\input{sjlogs/ex_choropleth_map.log.tex}\nullskip
\end{stlog}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{figs/wbopendata_example01.pdf}
\caption{Mobile cellular subscriptions per 100 people (latest available year). Created by merging \texttt{wbopendata} output with geographic shape data and visualizing using the \texttt{spmap} command \citep{pisati2007spmap}.}
\label{fig:map}
\end{figure}

\subsubsection{Alternative scatter plots (verbatim output)}
\begin{stlog}
\input{sjlogs/ex_scatter_poverty_gdp.log.tex}\nullskip
\end{stlog}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{figs/wbopendata_example04.pdf}
\caption{Poverty headcount ratio versus GDP per capita (PPP, constant international \textdollar). Regional aggregates are labeled; lowess smoother shows the cross-country relationship.}
\label{fig:scatter}
\end{figure}

% ---------------------------------------------------------------------
% A.4 User-written extensions (worldstat logs)
% ---------------------------------------------------------------------
\subsection{User-written extensions}\label{app:examples:extensions}

This subsection documents examples using user-written commands that rely on
\texttt{wbopendata} internally.

\subsubsection{\texttt{worldstat}: Africa example (verbatim output)}
\begin{stlog}
\input{sjlogs/ex_worldstat_africa.log.tex}\nullskip
\end{stlog}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{figs/wbopendata_worldstat_africa_gdp.pdf}
\caption{GDP per capita (constant 2015 US\textdollar) in Africa, 2009. Generated using \texttt{worldstat} \citep{clarke2012worldstat}, which calls \texttt{wbopendata} internally.}
\label{fig:worldstat_africa}
\end{figure}

\subsubsection{\texttt{worldstat}: global example (verbatim output)}
\begin{stlog}
\input{sjlogs/ex_worldstat_world.log.tex}\nullskip
\end{stlog}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{figs/wbopendata_worldstat_world_fertility.pdf}
\caption{Fertility rate (births per woman) worldwide. The \texttt{worldstat} command uses \texttt{wbopendata} internally to retrieve World Bank indicators.}
\label{fig:worldstat_fertility}
\end{figure}

\subsection{Notes on reproducibility}
All examples in this appendix are fully reproducible using the version of
\texttt{wbopendata} documented in the main text. Because outputs depend on live
API responses, results may differ over time as underlying data are revised.
Such differences reflect upstream updates rather than changes in analytical
logic and can be identified by re-running the corresponding commands.

% =====================================================================
% Appendix: Diagnostics and verbatim outputs
% =====================================================================
\section{Diagnostics and verbatim outputs}\label{app:diagnostics}

\subsection{Missing indicator example}
\begin{stlog}
\input{sjlogs/ex_indicator_missing.log.tex}\nullskip
\end{stlog}

\subsection{Deprecated / archived indicator example}
\begin{stlog}
\input{sjlogs/ex_indicator_deprecated.log.tex}\nullskip
\end{stlog}

\subsection{Metadata cache update output}
\begin{stlog}
\input{sjlogs/ex_update.log.tex}\nullskip
\end{stlog}

% =====================================================================
\section{Quality assurance, testing, and operational validation}\label{app:qa}
% =====================================================================

This appendix documents the quality assurance (QA) and testing framework used
to maintain the reliability of \texttt{wbopendata} over time. These practices
are a core component of the command’s reproducibility guarantees, ensuring that
changes in upstream data infrastructure, metadata, and indicator availability
are detected, diagnosed, and addressed explicitly.

Unlike statistical reproducibility, which concerns the stability of estimates
conditional on data, QA for data-access infrastructure must address a distinct
set of risks: API schema changes, endpoint deprecations, evolving indicator
vocabularies, pagination failures, and environment-specific constraints. The
framework described here is designed to surface such issues under real-world
operating conditions.

\subsection{Testing philosophy: operational validation}

The testing strategy for \texttt{wbopendata} is based on \emph{live integration
tests} that exercise the command end-to-end against the World Bank Data API.
Tests intentionally rely on live network access and real data downloads, rather
than mocked responses or static fixtures.

This approach differs from the release-gate testing paradigms used by CRAN or
PyPI, where deterministic, offline tests are required to certify software
correctness in sandboxed environments. In contrast, \texttt{wbopendata} operates
in trusted, interactive Stata environments where network access is both
available and essential. Because the core functionality of the command is to
retrieve and process live data, operational validation against the actual API
is the most informative way to assess reliability.

Under this paradigm, test failures do not necessarily imply software defects.
They may reflect upstream API outages, schema changes, or metadata revisions.
The objective of the test suite is therefore not to freeze outputs, but to make
such changes explicit and diagnosable.

\subsection{Test harness structure}

Automated tests are implemented in a single driver script,
\texttt{run\_tests.do}, which supports full test runs, single-test execution,
and verbose (trace) modes. Tests are organized into functional categories that
correspond to core features of the command, including environment checks,
download modes, output formatting, country metadata, graph metadata, update
commands, language options, and advanced features.

Each test follows a standardized pattern:
\begin{itemize}
  \item explicit execution of the command under test,
  \item immediate inspection of return codes,
  \item verification of expected variables, structures, or scalars,
  \item informative pass/fail messages tied to specific failure modes.
\end{itemize}

Helper routines manage test execution, result aggregation, and logging, ensuring
that failures are attributed to specific test identifiers. A complete test run
produces a timestamped log and updates a cumulative test history file.

\subsection{Regression testing and cumulative reliability}

A key feature of the QA framework is the systematic treatment of historical
bugs as permanent regression tests. Issues such as failures in multi-indicator
queries, metadata parsing errors, incorrect handling of deprecated indicators,
or option incompatibilities are encoded as named test cases (e.g., \texttt{REG-33},
\texttt{REG-45}, \texttt{REG-46}, \texttt{REG-51}).

This approach ensures that fixes are not transient. Once a bug is resolved, the
corresponding test remains in the suite and must pass in all subsequent runs.
As a result, reliability is cumulative rather than episodic.

The longitudinal test history recorded in \texttt{test\_history.txt} documents
this process explicitly: early versions show multiple failures as new features
were introduced and edge cases identified, followed by successive reductions in
failures as fixes were applied, culminating in fully passing test runs for
recent releases. This history provides an auditable record of stabilization over
time rather than a single-point snapshot.

\subsection{Diagnostics and failure modes}

Tests are designed to capture and report informative diagnostics rather than
binary outcomes. When a test fails, logs record the API endpoint accessed, the
parameters supplied, and the return codes or error messages produced. This
information is essential for distinguishing between upstream changes and genuine
software regressions.

Illustrative examples of user-facing diagnostics—including missing indicators,
deprecated series, and metadata update behavior—are reported in Appendix~\ref{app:diagnostics}.
Appendix~\ref{app:diagnostics} documents \emph{what users see}, while the present appendix documents
\emph{how those behaviors are systematically validated}.

\subsection{Indicator lifecycle and metadata evolution}

The QA framework explicitly accounts for the fact that the set of available
indicator codes and their associated metadata evolve over time. Indicators may
be deprecated, archived, or reclassified as source databases are revised, and
metadata fields may be corrected or expanded.

Tests covering update commands (\texttt{update query}, \texttt{update check},
\texttt{update all}) verify that such changes are detected and propagated through
the local cache. Deprecated indicators are expected to trigger informative
diagnostics rather than silent failures. By treating indicator lifecycle events
as testable behavior, the framework reduces the risk that analytical pipelines
continue to rely unknowingly on obsolete series or outdated definitions.

\subsection{Reproducibility of the QA process}

All QA artifacts—including test scripts, protocols, logs, and cumulative test
histories—are version-controlled alongside the source code. This allows the QA
process itself to be reproduced, inspected, and audited.

Because tests depend on live API responses, exact outputs may change as upstream
data are revised. Such changes are treated as expected signals when they reflect
documented updates. The purpose of the QA framework is therefore not to enforce
static results, but to ensure that change is explicit, traceable, and consistent
with upstream revisions.

\subsection{Continuous validation under ecosystem constraints}

Although \texttt{wbopendata} is developed within the Stata ecosystem, which
lacks the automated release-gate infrastructure common to CRAN or PyPI, its QA
framework implements the functional equivalent of continuous integration and
continuous validation. Automated test execution, regression protection, and
versioned test histories ensure that each code change is evaluated against both
current functionality and historically fixed issues.

In this setting, continuous validation is achieved not through centralized
CI/CD services, but through reproducible test scripts that can be executed
consistently by developers prior to release. Each test run produces a
timestamped log and updates a cumulative history, creating an auditable trail
of stability across versions. This approach aligns with institutional
constraints—such as the need for live API access and trusted execution
environments—while preserving the core objective of CI/CD practices: early
detection of regressions and disciplined release management.

By adapting CI/CD principles to the realities of statistical software
distribution, \texttt{wbopendata} demonstrates that rigorous quality assurance
is achievable even in environments without formal automation pipelines.

\subsection{Illustrative QA artifacts}

To support transparency, selected excerpts from the automated test harness and
logs are included below. These excerpts illustrate the structure of the testing
framework and the form of diagnostics produced during execution.

\subsubsection{Test harness initialization (excerpt)}
\begin{stlog}
\input{sjlogs/run_tests_excerpt.log.tex}\nullskip
\end{stlog}

\subsubsection{Summary of a complete test run (excerpt)}
\begin{stlog}
\input{sjlogs/run_tests_summary_excerpt.log.tex}\nullskip
\end{stlog}

\subsection{Implications for reproducible analytics}

The QA practices documented here reinforce the paper’s central argument that
reproducibility depends not only on statistical methods, but also on the
reliability of data-access infrastructure. By embedding operational validation,
regression testing, and explicit diagnostics into the development lifecycle,
\texttt{wbopendata} ensures that scripted data acquisition remains a stable and
auditable foundation for empirical analysis, even as upstream data systems
evolve.

In this sense, quality assurance is not an ancillary engineering concern, but a
core component of reproducible analytics. It provides the enforcement mechanism
through which principles such as data acquisition as code and explicit data
provenance are sustained in practice.


\end{document}
